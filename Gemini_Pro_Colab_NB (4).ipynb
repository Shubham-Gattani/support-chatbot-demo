{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install and Import dependencies"
      ],
      "metadata": {
        "id": "eLfXR5wBVPVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZRNbiPzkxr7",
        "outputId": "298842af-cffc-4b01-a907-9ae9ef5ea3a5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m526.1/526.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n",
            "Requirement already satisfied: langchain_google_vertexai in /usr/local/lib/python3.11/dist-packages (2.0.18)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.81.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (1.84.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (2.19.0)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (0.4.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (0.3.49)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.9 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (2.10.6)\n",
            "Requirement already satisfied: validators<0.35.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai) (0.34.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (2.24.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (5.29.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (24.2)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (3.29.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (4.12.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (1.7.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (0.14.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (4.9)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (0.14.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.49->langchain_google_vertexai) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.11/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (2.2.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.81.0->langchain_google_vertexai) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --quiet langchain langchain_community chromadb  langchain-google-vertexai\n",
        "!pip install --quiet \"unstructured[all-docs]\" pypdf pillow pydantic lxml pillow matplotlib chromadb tiktoken\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade langchain_google_vertexai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we are specifying the project ID of GCP, and credentials that you can request either Shubham or Puneet."
      ],
      "metadata": {
        "id": "aqKEBVpOVWkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "PROJECT_ID = \"rag-demo-454104\"\n",
        "REGION = \"asia-south1\"\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/rag-demo-454104-9fdb4300f7d3.json\""
      ],
      "metadata": {
        "id": "hySYIDZkoYV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "vertexai.init(project = PROJECT_ID , location = REGION) # This line is used to initialize the Vertex AI API for the given project and location"
      ],
      "metadata": {
        "id": "ALKeR-M8oes_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prepare and load data"
      ],
      "metadata": {
        "id": "lOQ6J6QmVk95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader2 = PyPDFLoader(\"/content/Student_Handbook.pdf\")\n",
        "docs2 = loader2.load()\n",
        "\n",
        "docs = docs2\n",
        "\n",
        "texts = [d.page_content for d in docs]\n",
        "\n",
        "tables = [] # QUESTION: Where is the code to process tables that are there in the PDF?"
      ],
      "metadata": {
        "id": "mXV0DypaooND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts) # Meaning there are total 36 pages in the PDF docs that we uploaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tpftTxFqYUB",
        "outputId": "b46d0f00-e019-4044-8291-9f3b4fd3a3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[5] # This is the actual content of the sixth page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "HDcG6dvRqcdL",
        "outputId": "1ca0e484-cc5a-4abf-94e4-2094270f6fe6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.   Fees for the entireprogramme\\nâ— First year fees kept low to enable learners to try out theprogramme\\nâ— Entry fee for Qualifier exam: Rs. 3000 (non-refundable)- with suitable waivers\\nâ— Fee waivers for learners belonging to certain categoriesand economic backgrounds\\nNumber of credits in each level:\\nLevel Theory Project Apprenticeship NPTEL MG/HS/HM\\nFoundation 32\\nDiploma inDS 23 4\\nDiploma inP 23 4\\nBSc 28-X X (0-4)\\nBS 28-A-B A (0/4/12) B=4\\n*Only maximum of 24 credits can be transferredtowards non counted CGPA course\\nFoundation: Rs 32000/-Diploma Level: 62500 x 2 = Rs 125,000/-BSc Level: Rs 2.21L - 2.47L        BS Level: 3.25L - 3.87L\\nCostpercredit(Rs)\\nNumber ofcredits Total fee Levelfees\\nFoundation 1000/c 32c 32000 32,000\\nDiploma 2500/cforTheory1250/cforproject\\n23x2=46c4x2 = 8c 115,00010,000 125,000\\nBSc Level 3- Rs2500/cLevel 4- Rs5000/c1000/cforNPTEL\\n20c for 5mandatorycourses(Level 3)4cNPTEL+4cLevel 34c NPTEL+ 4c Level48c Level 34c Level 3+ 4c Level48c Level 4\\n500004000+100004000+20000200003000040000\\n64,00074,00070,00080,00090,000\\nBS Level 3- Rs2500/cLevel 4- Rs5000/c1000/cforNPTEL\\n2 Level 4mandatoryDS = 8c2 Level 4mandatoryDP = 8c4c NPTEL+ 8c Level34c NPTEL+ 4c Level3 + 4cLevel 44c NPTEL+ 8c Level412c Level312c Level4\\n40000400004000+200004000+300004000+400003000060000\\n104000114000124000110000140000\\nPublished using Google Docs Learn more\\nIITM BS Degree Programme - Student Handbook (Updated 07032025) Updated automatically every 5 minutes\\n3/18/25, 10:26 AM IITM BS Degree Programme - Student Handbook (Updated 07032025)\\nhttps://docs.google.com/document/u/1/d/e/2PACX-1vQB7SYIXQPJr0-WcfekVVSt488MdlkNzRUPacbRh2QgOALXcinPybopWIFlY83tdr_mH1Qtrâ€¦ 6/36'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7mkf-SlqgW-",
        "outputId": "689c6d6b-0fbb-4ef2-f1e8-28bacb5ea54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Generate Text Summaries"
      ],
      "metadata": {
        "id": "naNDeR1oVvHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAI, ChatVertexAI, VertexAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda"
      ],
      "metadata": {
        "id": "hZ-MmJ1uquD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
        "    \"\"\"\n",
        "    Summarize text elements\n",
        "    texts: List of str\n",
        "    tables: List of str\n",
        "    summarize_texts: Bool to summarize texts\n",
        "    \"\"\"\n",
        "    prompt_text = \"\"\"\n",
        "    You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(prompt_text)\n",
        "\n",
        "    empty_response = RunnableLambda(\n",
        "        lambda x: AIMessage(content=\"Error processing document\")\n",
        "    )\n",
        "\n",
        "    model_name = \"gemini-pro\"\n",
        "    model = VertexAI(\n",
        "        temperature=0, model_name=model_name, max_output_tokens=1024\n",
        "    ).with_fallbacks([empty_response])\n",
        "\n",
        "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "    text_summaries = []\n",
        "    table_summaries = []\n",
        "\n",
        "    if texts and summarize_texts:\n",
        "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
        "    elif texts:\n",
        "        text_summaries = texts\n",
        "\n",
        "    if tables:\n",
        "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
        "\n",
        "    return text_summaries, table_summaries"
      ],
      "metadata": {
        "id": "M6nBgnVe2wBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summaries, table_summaries = generate_text_summaries(\n",
        "    texts, tables, summarize_texts=False\n",
        ") # For 10 page PDF, it takes 37 seconds and 2 WARNINGS; 4m37s for full 36 page PDF; and also didn't use any money\n",
        "# When summarize_texts=False, the code runs instantly..cool!"
      ],
      "metadata": {
        "id": "V3-HJh7O2yFt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import pickle\n",
        "# with open('/content/drive/My Drive/summaries.pkl', 'wb') as f:\n",
        "#     pickle.dump([text_summaries, table_summaries], f)\n",
        "\n",
        "# import pickle # Load later\n",
        "# with open('/content/drive/My Drive/summaries.pkl', 'rb') as f:\n",
        "#     text_summaries, table_summaries = pickle.load(f)"
      ],
      "metadata": {
        "id": "2F8kbkOTqR0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "(text_summaries[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "y_p5Vl09vMkB",
        "outputId": "ce54d01c-6bd7-44be-d02b-833721e0d08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Student Handbook\\n        IITM BS Degree Office, 3rd Floor, ICSR Building, IIT Madras, Chennai - 600036                             support@study.iitm.ac.in\\nPlease read this\\nDear student,\\nThis Student Handbook is intended to provide IIT MadrasBS Degree students with the information and policies theyshould be aware of, which may help them make the mostof the opportunities offered in this programme. It also givesyou formal notification and explanation of the programmeâ€™sregulations, policies and procedures.\\nIt is essential, and your responsibility, to read it andfamiliarize yourself with the content. It should be kepthandy and referred to during your time with us.\\nThis handbook may be revised time and again. While thisstudent handbook was prepared on the basis of the bestinformation available at the time of publication, allinformation, including policies and procedures are subjectto change without notice or obligation.\\nAll rules and regulations in this document are approved byour committee. As changes keep getting approved, we willupdate them here.We hope that your time as a student in this programme issuccessful and enjoyable.\\nThank you.\\nTable of Contents\\nPublished using Google Docs Learn more\\nIITM BS Degree Programme - Student Handbook (Updated 07032025) Updated automatically every 5 minutes\\n3/18/25, 10:26 AM IITM BS Degree Programme - Student Handbook (Updated 07032025)\\nhttps://docs.google.com/document/u/1/d/e/2PACX-1vQB7SYIXQPJr0-WcfekVVSt488MdlkNzRUPacbRh2QgOALXcinPybopWIFlY83tdr_mH1Qtrâ€¦ 1/36'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "193jAYwwhrOO",
        "outputId": "d879b642-b25f-4368-d611-6330abdd4017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Generate Image Summaries"
      ],
      "metadata": {
        "id": "f7IMW1ApV0yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_vertexai import ChatVertexAI"
      ],
      "metadata": {
        "id": "myrlLBDnQDr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path): # This function encodes the image to base64 string\n",
        "    \"\"\"Getting the base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "    \"\"\"Generates a textual summary of an image using the ChatVertexAI model with the 'gemini-pro-vision' capability.\"\"\"\n",
        "    model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
        "\n",
        "    msg = model(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        "\n",
        "\n",
        "def generate_img_summaries(path):\n",
        "    \"\"\"\n",
        "    Generate summaries and base64 encoded strings for images\n",
        "    path: Path to list of .jpg files extracted by Unstructured\n",
        "    \"\"\"\n",
        "\n",
        "    img_base64_list = []\n",
        "\n",
        "    image_summaries = []\n",
        "\n",
        "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw image. \\\n",
        "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "    for img_file in sorted(os.listdir(path)):\n",
        "        if img_file.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(path, img_file)\n",
        "            base64_image = encode_image(img_path)\n",
        "            img_base64_list.append(base64_image)\n",
        "            image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "    return img_base64_list, image_summaries\n",
        "\n",
        "img_base64_list, image_summaries = generate_img_summaries(\"/content/\")\n",
        "\n",
        "print(f\"Number of image summaries: {len(image_summaries)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmGRrMq6QJDG",
        "outputId": "937c8649-6e6d-4871-c790-ec36a118ec56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-a9c60bb58a06>:11: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  msg = model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image summaries: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Summary 1: {image_summaries[0]}\")\n",
        "print(f\"Summary 2: {image_summaries[1]}\")\n",
        "print(f\"Summary 3: {image_summaries[2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEx-2ISUQdiw",
        "outputId": "1e453438-db52-4d99-a4e9-d139fc4d74a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary 1:  The image is a table that shows the cost per credit, number of credits, total fee, and level fees for various courses. The cost per credit is in rupees, and the number of credits is listed next to it. The total fee is calculated by multiplying the cost per credit by the number of credits. The level fees are listed in the last column.\n",
            "Summary 2:  The image is a table that shows the number of credits required for each level of study in a particular program. The levels are Foundation, Diploma in Data Science (DS), Diploma in Pharmacy (P), Bachelor of Science (BSc), and Bachelor of Studies (BS). For each level, the number of credits required for theory, project, apprenticeship, NPTEL, and MG/HS/HM are listed.\n",
            "Summary 3:  The image describes the course structure of a Bachelor of Science in Data Science and Application program at the Indian Institute of Technology Madras. The program consists of three levels: Foundation, Diploma, and Degree. The Foundation level has 8 courses and 32 credits. The Diploma level has 12 courses, 4 projects, and 54 credits. The Degree level has 7 courses and 28 credits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Build Multi-Vector Retrieval"
      ],
      "metadata": {
        "id": "xoFf3YkiV8oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a multi-vector retriever?\n",
        "\n",
        "Alright, let's make this fun and simple!\n",
        "\n",
        "Imagine you have a big box of toys, but the toys are all mixed up. Some are action figures, some are puzzle pieces, and others are crayons. One day, your friend asks, \"Can you find me something to draw with?\" and another asks, \"Can you find me a toy to play with?\"\n",
        "\n",
        "A **multi-vector retriever** is like a super-smart robot helper. It doesn't just search for one type of toy (like crayons) but knows how to look for all kinds of toys (action figures, puzzle pieces, and crayons) at the same time. It organizes the toys into groups based on what theyâ€™re good for, and then, when someone asks for a specific kind of toy, it quickly fetches the best match!\n",
        "\n",
        "### Example in Action\n",
        "Say you have:\n",
        "- **Text**: A storybook\n",
        "- **Pictures**: A drawing of a dinosaur\n",
        "- **Tables**: A list of your favorite snacks\n",
        "\n",
        "If someone asks, \"Whatâ€™s the scariest dinosaur youâ€™ve seen?\", the multi-vector retriever would:\n",
        "1. Look in the storybook for mentions of \"dinosaurs.\"\n",
        "2. Search the drawing for a picture of a dinosaur.\n",
        "3. Quickly fetch that information, so you can answer: â€œA scary T-Rex!â€\n",
        "\n",
        "So, itâ€™s like having a helper who can find answers no matter what form the information is inâ€”text, pictures, or lists. Cool, right? ğŸ˜Š"
      ],
      "metadata": {
        "id": "CzdEUdB7hJWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_vertexai import VertexAIEmbeddings"
      ],
      "metadata": {
        "id": "AM7qRbolRUuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multi_vector_retriever(\n",
        "   vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
        "):\n",
        "   \"\"\"\n",
        "   Create retriever that indexes summaries, but returns raw images or texts.\n",
        "   \"\"\"\n",
        "   store = InMemoryStore()\n",
        "   id_key = \"doc_id\"\n",
        "\n",
        "   retriever = MultiVectorRetriever(\n",
        "       vectorstore=vectorstore,\n",
        "       docstore=store,\n",
        "       id_key=id_key,\n",
        "   )\n",
        "\n",
        "   def add_documents(retriever, doc_summaries, doc_contents):\n",
        "       doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "       summary_docs = [\n",
        "           Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "           for i, s in enumerate(doc_summaries)\n",
        "       ]\n",
        "       retriever.vectorstore.add_documents(summary_docs)\n",
        "       retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "   if text_summaries:\n",
        "       add_documents(retriever, text_summaries, texts)\n",
        "   if table_summaries:\n",
        "       add_documents(retriever, table_summaries, tables)\n",
        "   if image_summaries:\n",
        "       add_documents(retriever, image_summaries, images)\n",
        "\n",
        "   return retriever\n",
        "\n",
        "vectorstore = Chroma(\n",
        "   collection_name=\"mm_rag_cj_blog\",\n",
        "   embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"), # Think of this as the tool that turns your information into special \"codes\" (vectors) the digital bookshelf can understand. Here, itâ€™s using VertexAIEmbeddings, a model designed to create these codes based on the latest technology (textembedding-gecko@latest).\n",
        ")\n",
        "\"\"\"\n",
        "Imagine Chroma as a digital bookshelf that stores summaries or information in a way that makes it super easy to search later. Each piece of information (like text or image summaries) is turned into a \"vector\" (a mathematical representation) so the system can find things quickly.\n",
        "\"\"\"\n",
        "\n",
        "retriever_multi_vector_img = create_multi_vector_retriever(\n",
        "   vectorstore,\n",
        "   text_summaries,\n",
        "   texts,\n",
        "   table_summaries,\n",
        "   tables,\n",
        "   image_summaries,\n",
        "   img_base64_list,\n",
        ")"
      ],
      "metadata": {
        "id": "Usje-HqqRoTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba31f2e-05f8-4006-d34e-9e6e4948aaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-058bfde9a57d>:34: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function **`create_multi_vector_retriever`** is like a librarian who organizes summaries of books, images, and tables in a library but gives you the actual book or image whenyou ask for it.\n",
        "\n",
        "### Explanation:\n",
        "- **Indexing summaries**: Imagine writing a short note about each book in the library to remember what itâ€™s about. This is like creating summaries for texts, tables, or images.\n",
        "- **Returning raw content**: But when someone wants to borrow the book (or image), the librarian gives them the full book, not just the note.\n",
        "\n",
        "### Example:\n",
        "Imagine you have:\n",
        "1. A **storybook** with the summary: *\"A magical adventure with a brave wizard.\"*\n",
        "2. A **table** with the summary: *\"A list of magical creatures.\"*\n",
        "3. An **image** of a dragon, with the summary: *\"A fierce red dragon flying over mountains.\"*\n",
        "\n",
        "Hereâ€™s how it works:\n",
        "1. All these summaries are stored in a database so that theyâ€™re easy to search.\n",
        "2. If you ask for \"Tell me about dragons,\" the retriever finds the dragon imageâ€™s summary based on its description.\n",
        "3. Instead of just giving you the summary (*\"A fierce red dragon flying over mountains\"*), it retrieves and returns the **actual image of the dragon**.\n",
        "\n",
        "In short, the retriever uses summaries to locate things but delivers the original content. This approach is helpful in multimodal systems where users interact with different kinds of data like text, images, or tables!"
      ],
      "metadata": {
        "id": "hjAQAM6FsERE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does vector store has actual columns?\n",
        "\n",
        "Instead of thinking about the vectorstore as a database with actual \"columns,\" it's better to think of it as:\n",
        "\n",
        "- A search-optimized system that stores embeddings.\n",
        "\n",
        "- It uses **metadata (like document IDs)** to connect to other storage systems (e.g., docstore), which hold summaries and raw content."
      ],
      "metadata": {
        "id": "Q547Yvamzo8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Building Multi-Modal RAG"
      ],
      "metadata": {
        "id": "nes1wDaXWHae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "import base64\n",
        "from IPython.display import HTML, display\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from PIL import Image\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "y6YeptQ3R0e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_img_base64(img_base64):\n",
        "    \"\"\"This function takes a base64 encoded image string (img_base64) and displays it as an image within the Jupyter Notebook.\"\"\"\n",
        "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />' #creates an HTML <img> tag with the src attribute set to the base64 encoded image data.\n",
        "    display(HTML(image_html))\n",
        "\n",
        "\n",
        "def looks_like_base64(sb):\n",
        "    \"\"\"Check if the string looks like base64\"\"\"\n",
        "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
        "\n",
        "\n",
        "def is_image_data(b64data):\n",
        "    \"\"\"\n",
        "    Check if the base64 data is an image by looking at the start of the data\n",
        "    \"\"\"\n",
        "    # define a dictionary image_signatures containing the characteristic byte sequences (signatures) found at the beginning of different image file formats (JPG, PNG, GIF, WEBP).\n",
        "    image_signatures = {\n",
        "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
        "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
        "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "    }\n",
        "    try:\n",
        "        header = base64.b64decode(b64data)[:8]\n",
        "        for sig, format in image_signatures.items():\n",
        "            if header.startswith(sig):\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def resize_base64_image(base64_string, size=(128, 128)):\n",
        "    \"\"\"\n",
        " This function takes a base64 encoded image string and resizes the image to the specified dimensions, returning the resized image as a base64 encoded string.\n",
        "\n",
        "    \"\"\"\n",
        "    img_data = base64.b64decode(base64_string)\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    resized_img = img.resize(size, Image.LANCZOS)\n",
        "\n",
        "    buffered = io.BytesIO()\n",
        "    resized_img.save(buffered, format=img.format)\n",
        "\n",
        "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "    \"\"\"\n",
        "This function takes a list of documents (docs) and separates them into two categories: base64-encoded images and text content. It then returns a dictionary containing these separated elements.\n",
        "\"\"\"\n",
        "    b64_images = []\n",
        "    texts = []\n",
        "    for doc in docs:\n",
        "        if isinstance(doc, Document):\n",
        "            doc = doc.page_content\n",
        "        if looks_like_base64(doc) and is_image_data(doc):\n",
        "            doc = resize_base64_image(doc, size=(1300, 600))\n",
        "            b64_images.append(doc)\n",
        "        else:\n",
        "            texts.append(doc)\n",
        "    if len(b64_images) > 0:\n",
        "        return {\"images\": b64_images[:1], \"texts\": []}\n",
        "    return {\"images\": b64_images, \"texts\": texts}\n",
        "\n",
        "\n",
        "def img_prompt_func(data_dict):\n",
        "    \"\"\"\n",
        "    This function takes a dictionary (data_dict) containing the context (texts and images) and the user's question. It formats this information into a prompt suitable for a multi-modal language model, specifically for the ChatVertexAI model.\n",
        "    \"\"\"\n",
        "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"]) # This line joins any text elements from the data_dict into a single string, separated by newlines.\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"\n",
        "      You are a professional support person of IITM BS Degree tasked with providing help.\n",
        "      You will be given a mixture of text, tables, and image(s) usually of charts or graphs.\n",
        "      Use this information to provide help related to the user question.\n",
        "      Make sure your response doesn't contain any information outside the provided context.\n",
        "      If the information is not available in the provided context, say \"I'm sorry, but I couldn't find information about that in the document.\" Response should be in a markdown format in bullet points if needed.\n",
        "      \"\"\"\n",
        "    messages = [] # Initializes an empty list to store prompt messages.\n",
        "\n",
        "    text_message = {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": (\n",
        "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "            \"Text and/or tables:\\n\"\n",
        "            f\"{formatted_texts}\"\n",
        "        ),\n",
        "    }\n",
        "    messages.append(text_message)\n",
        "\n",
        "    if data_dict[\"context\"][\"images\"]:\n",
        "        for image in data_dict[\"context\"][\"images\"]:\n",
        "            image_message = {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "            }\n",
        "            messages.append(image_message)\n",
        "\n",
        "    return [SYSTEM_PROMPT, HumanMessage(content=messages)]"
      ],
      "metadata": {
        "id": "RB9A0I65SCVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_rag_chain(retriever):\n",
        "    \"\"\"\n",
        "    This function constructs a multi-modal (RAG) chain. This chain orchestrates the process of retrieving relevant information from a knowledge base and using it to answer user questions, handling both text and images.\n",
        "    \"\"\"\n",
        "    model = ChatVertexAI(\n",
        "        temperature=0, #  Controls the randomness of the model's output (0 means deterministic).\n",
        "        model_name=\"gemini-pro-vision\", # Specifies the model to use, likely one capable of handling both text and images.\n",
        "        max_output_tokens=1024, # max_output_tokens=1024: Limits the length of the model's response.\n",
        "\n",
        "    )\n",
        "\n",
        "    chain = ( # Defines the RAG chain using a series of steps\n",
        "        {\n",
        "            \"context\": retriever | RunnableLambda(split_image_text_types), # This part handles retrieving relevant context from the knowledge base.\n",
        "            #\"retriever\" is the retriever object (likely a MultiVectorRetriever) used to search the knowledge base.\n",
        "            # RunnableLambda(split_image_text_types): This function is applied to the retrieved documents to separate images and texts.\n",
        "            \"question\": RunnablePassthrough(), # This simply passes the user's question through to the next step.\n",
        "        }\n",
        "        | RunnableLambda(img_prompt_func) # This function takes the retrieved context and the question and formats them into a prompt for the language model.\n",
        "        | model # This is the ChatVertexAI language model, which processes the prompt and generates an answer\n",
        "        | StrOutputParser() # This parses the model's output into a string.\n",
        "    )\n",
        "\n",
        "    return chain"
      ],
      "metadata": {
        "id": "xWtpPjPFfpqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine a librarian who can find information in books and images to answer your questions. This function is like creating a set of instructions for the librarian:\n",
        "\n",
        "1. Find relevant information: The librarian uses a search tool (retriever) to locate books and images related to your question.\n",
        "2. Organize the information: The librarian separates the text from the images to make it easier to understand.\n",
        "3. Prepare a summary: The librarian summarizes the relevant information from the books and images in a way that's easy for you to understand.\n",
        "4. Answer your question: The librarian uses their knowledge and the prepared summary to answer your question.\n",
        "\n",
        "The multi_modal_rag_chain function does something similar, but with a language model instead of a librarian."
      ],
      "metadata": {
        "id": "4-h7n7TcJZtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
      ],
      "metadata": {
        "id": "OHokkRHVSLxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatVertexAI?"
      ],
      "metadata": {
        "id": "bwe8TDsdt-f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Testing and Evaluation"
      ],
      "metadata": {
        "id": "TGTgbMw0WTZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown as md\n",
        "# query = \"Explain the fee structure for each level of the program, including fee waivers and additional costs.\"\n",
        "query = \"What are the BSc and BS credit requirements in IITM BS Degree?\"\n",
        "# query = \"Explain the fee structure for each level of the program, including fee waivers.\"\n",
        "\n",
        "# docs = retriever_multi_vector_img.invoke(query, limit=1)\n",
        "\n",
        "# print(f\"Number of relevant documents retrieved: {len(docs)}\")\n",
        "\n",
        "# print(docs, end=\"\\n\")\n",
        "# print(type(docs))\n",
        "result = chain_multimodal_rag.invoke(query)\n",
        "# md(result)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "etHBE4-bSvNp",
        "outputId": "f6d4d691-50a4-4f5b-e7ec-dcef6e8f23d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- The number of credits required to graduate with the BSc is 114 while that for the BS is 142.\\n- The structure of the program is as follows:\\n  - Level 1: The Foundation level (32 credits)\\n  - Level 2: Diploma level (54 credits)\\n  - Level 3: BSc Degree level courses (28 credits)\\n  - Level 4: BS Degree level courses (28 credits)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result = chain_multimodal_rag.invoke(query)\n",
        "\n",
        "# # from IPython.display import Markdown as md\n",
        "# # md(result)\n",
        "# result"
      ],
      "metadata": {
        "id": "hmyDDaYuUPGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLE QUESTIONS"
      ],
      "metadata": {
        "id": "ncL0yYr7yY_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q1. What are the BSc and BS credit requirements in IITM BS Degree?**\n",
        "\n",
        "- The number of credits required to graduate with the BSc is 114 while that for the BS is 142.\n",
        "- The structure of the program is as follows:\n",
        "  - Level 1: The Foundation level (32 credits)\n",
        "  - Level 2: Diploma level (54 credits)\n",
        "  - Level 3: BSc Degree level courses (28 credits)\n",
        "  - Level 4: BS Degree level courses (28 credits)"
      ],
      "metadata": {
        "id": "i3V5vzxLwiBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q2. What are the different exit options available in the IIT Madras BS Degree program?** (CORRECT ANSWER)\n",
        " There are multiple exit points in the IIT Madras BS in Data Science & Applications program.\n",
        "1. After completing the foundation level, you can exit with a Foundation Certificate from ITM CODE. **(ATTENTION: wtf is this ITM CODE ğŸ˜†)**\n",
        "2. After completing the diploma level, you can exit with a Diploma in Programming and/or a Diploma in Data Science from IIT Madras.\n",
        "3. After completing the degree level (BSc), you can exit with a BSc Degree in Programming & Data Science from IIT Madras.\n",
        "4. After completing the degree level (BS), you can exit with a BS Degree in Data Science and Application from IIT Madras."
      ],
      "metadata": {
        "id": "uviua3ImxH9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q3: What is the eligibility to apply for the Qualifier Exam?** (CORRECT)\n",
        "To be eligible to apply for the Qualifier Exam, you must meet the following criteria:\n",
        "\n",
        "1. Have studied Mathematics and English in Class 10.\n",
        "2. Have passed Class 12 or equivalent, regardless of age or academic background.\n",
        "3. School students who have appeared for their Class 11 final exams can apply irrespective of their group/stream/board."
      ],
      "metadata": {
        "id": "-kDNXmDAxgWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q4. How many credits are required to complete the BS degree in Data Science & Applications?** (INCORRECT)\n",
        " To complete the BS degree in Data Science & Applications, 128 credits are required."
      ],
      "metadata": {
        "id": "wGrcvbkgyDZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **In which courses are project required?** (!!!!!!!!!!!!)\n",
        " Projects are required in the following courses:\n",
        "1. Diploma in Data Science\n",
        "2. Diploma in Programming\n",
        "3. B.S."
      ],
      "metadata": {
        "id": "hfsegFf42w67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How does the credit system in the IIT Madras BS in Data Science & Applications program compare to the traditional IITM campus credit system?** **(ATTENTION: Hallucinated answer in both cases whether you use summary or not...very bad)**\n",
        " The credit system in the IIT Madras BS in Data Science & Applications program is different from the traditional IITM campus credit system. In the BS in Data Science & Applications program, students are required to complete 120 credits to earn a degree. These credits are distributed across four levels of study: Foundation Level (32 credits), Diploma Level (54 credits), Degree Level (28 credits), and Degree Level (28 credits).\n",
        "\n",
        "In the traditional IITM campus credit system, students are required to complete 180 credits to earn a degree. These credits are distributed across eight semesters of study.\n",
        "\n",
        "One of the key differences between the two credit systems is the number of credits required to complete a degree. In the BS in Data Science & Applications program, students are required to complete 120 credits, while in the traditional IITM campus credit system, students are required to complete 180 credits. This means that students in the BS in Data Science & Applications program will have to take fewer courses to earn a degree.\n",
        "\n",
        "Another key difference between the two credit systems is the distribution of credits across the different levels of study. In the BS in Data Science & Applications program, students are required to complete 32 credits at the Foundation Level, 54 credits at the Diploma Level, 28 credits at the Degree Level, and 28 credits at the Degree Level. In the traditional IITM campus credit system, students are required to complete 22 credits in the first year, 25 credits in the second year, 27 credits in the third year, and 26 credits in the fourth year. This means that students in the BS in Data Science & Applications program will have more flexibility in choosing their courses."
      ],
      "metadata": {
        "id": "FC0GffboyhSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How can I get a loan for this program? Please give any contact information that can be useful.**\n",
        "\n",
        "For any help with the SBI Loan Process, please contact Martina/Angelin at 9444020900/8608076093.\n"
      ],
      "metadata": {
        "id": "OBUyYHxZckAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explain the fee structure for each level of the program, including fee waivers and additional costs.** (Very complex answer in both cases, moreover there is NO mention of level 5 anywhere in the PDF, so totally incorrect answer)\n",
        "\n",
        " The fee structure for the IITM BS Degree program is as follows:\n",
        "\n",
        "Foundation Level: Rs. 1000 per credit, with a total of 32 credits required. This amounts to a total fee of Rs. 32,000.\n",
        "\n",
        "Diploma Level: Rs. 2500 per credit for theory courses and Rs. 10,000 for the project, with a total of 46 credits required (23x2=46 credits). This amounts to a total fee of Rs. 125,000.\n",
        "\n",
        "BSc Level 3: Rs. 2500 per credit for mandatory courses and Rs. 1000 per credit for NPTEL courses, with a total of 20 credits required (5 mandatory + 15 NPTEL). This amounts to a total fee of Rs. 64,000 (40000+20000+4000).\n",
        "\n",
        "BSc Level 4: Rs. 5000 per credit for Level 4 courses and Rs. 1000 per credit for NPTEL courses, with a total of 28 credits required (4 Level 4 + 4 Level 3 + 8 Level 4 + 8 Level 3 + 4 NPTEL). This amounts to a total fee of Rs. 70,000 (20000+32000+18000).\n",
        "\n",
        "BSc Level 5: Rs. 5000 per credit for Level 5 courses and Rs. 1000 per credit for NPTEL courses, with a total of 28 credits required (4 Level 5 + 4 Level 4 + 8 Level 5 + 8 Level 4 + 4 NPTEL). This amounts to a total fee of Rs. 90,000 (20000+32000+38000).\n",
        "\n",
        "BSc Level 6: Rs. 5000 per credit for Level 6 courses and Rs. 1000 per credit for NPTEL courses, with a total of 24 credits required (4 Level 6 + 4 Level 5 + 8 Level 6 + 8 Level 5 + 4 NPTEL). This amounts to a total fee of Rs. 80,000 (20000+32000+28000).\n",
        "\n",
        "In addition to the tuition fees, there are also a number of other fees that students may be required to pay, such as a registration fee, a library fee, and a technology fee. These fees vary depending on the level of study and the specific program.\n",
        "\n",
        "Students who are eligible for financial aid may be able to receive a fee waiver or reduction. For more information on financial aid, please visit the IITM BS Degree program website."
      ],
      "metadata": {
        "id": "rUO7BtCcdHn5"
      }
    }
  ]
}